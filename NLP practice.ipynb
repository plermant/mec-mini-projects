{
 "cells": [
  {
   "cell_type": "raw",
   "id": "10ac37c7",
   "metadata": {},
   "source": [
    "This is two-part application.\n",
    "The first part, model creation, is this Notebook, where different models are trained and saved to file.\n",
    "The second part is the streamilt application, which is pasted in the first cell below as a large comment.\n",
    "To run the app, perform \"streamlit run appName.py\", where appName .py is the code right below placed in a separate python file on the machine where the app is run. It generates an url user can paste in their browser to exercice the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83078310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application streamlit code, see details above\n",
    "# import os, pathlib, shutil, random\n",
    "# from pathlib import Path\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.layers import TextVectorization\n",
    "# import streamlit as st\n",
    "\n",
    "# st.title('Movie sentiment across models')\n",
    "\n",
    "# #Global variables\n",
    "# maxTokens = 20000 # max vocabulary size\n",
    "# max_length=600 # max document size\n",
    "# batch_size=32\n",
    "# nlpRoot=\"C:/Users/plermant/git/miniprojects/NLPpractice\"\n",
    "\n",
    "# trainPath=nlpRoot+\"/aclImdb/train\"\n",
    "\n",
    "# train_ds = keras.utils.text_dataset_from_directory(trainPath, batch_size=batch_size)\n",
    "# text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "# text_vectorizationTfIdf = TextVectorization(ngrams=2,max_tokens=maxTokens,output_mode=\"tf-idf\")\t\n",
    "# text_vectorizationTfIdf.adapt(text_only_train_ds)\n",
    "\n",
    "# text_vectorizationBidiLSTM = layers.TextVectorization(max_tokens=maxTokens,output_mode=\"int\",output_sequence_length=max_length,)\n",
    "# text_vectorizationBidiLSTM.adapt(text_only_train_ds)\n",
    "\n",
    "# def modelFromName(name):\n",
    "#     path=nlpRoot+\"/models/\"+name\n",
    "#     if name=='bagBigramTdIdf' or name =='embedBidiLSTM':\n",
    "#         return tf.keras.models.load_model(path)\t\n",
    "#     else:\n",
    "#         print(\"Error in model name, does not exist, aborting ...\")\n",
    "#         exit(1)\n",
    "        \n",
    "# def predictSentiment(inString,modelName):\n",
    "#     model=modelFromName(modelName)\n",
    "#     inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "#     if modelName=='bagBigramTdIdf':\n",
    "#         processed_inputs = text_vectorizationTfIdf(inputs)\n",
    "#     elif modelName=='embedBidiLSTM':\n",
    "#         processed_inputs = text_vectorizationBidiLSTM(inputs)\n",
    "#     outputs = model(processed_inputs)\n",
    "#     inference_model = keras.Model(inputs, outputs)\n",
    "#     predictions = inference_model(inString) \n",
    "#     resp=round(float(predictions[0] * 100),2)\n",
    "#     return resp\n",
    "\t\n",
    "# modelName= st.selectbox(\"Select a ML model from this list\",(\"bagBigramTdIdf\", 'embedBidiLSTM',\"more to come ...\"))\n",
    "# inputString= st.text_input('What do you think of the movie? e.g. \"I loved the movie\", or \"This movie sucked\"')\n",
    "\n",
    "# sentimentPercent=predictSentiment(tf.convert_to_tensor([[inputString],]),modelName)\n",
    "# st.write(\"Sentiment value is\",sentimentPercent,\"per\",modelName,\"model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61c667bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e38e85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "percentVal=.2\n",
    "batch_size = 32\n",
    "maxTokens = 20000 # max voclbulary size\n",
    "max_length = 600 # max number of words in document\n",
    "epochCount=2\n",
    "nlpRoot=\"C:/Users/plermant/git/miniprojects/NLPpractice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcefc0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch files and untar, to be performed only once in project\n",
    "#!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "#!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57d14252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put percentVal of samples in a validation directory, to be performed only once in project\n",
    "# base_dir = pathlib.Path(\"aclImdb\")\n",
    "# val_dir = base_dir / \"val\" \n",
    "# train_dir = base_dir / \"train\" \n",
    "# for category in (\"neg\", \"pos\"):\n",
    "#     os.makedirs(val_dir / category)\n",
    "#     files = os.listdir(train_dir / category)\n",
    "#     random.Random(1337).shuffle(files)\n",
    "#     num_val_samples = int(percentVal * len(files))\n",
    "#     val_files = files[-num_val_samples:]\n",
    "#     for fname in val_files:\n",
    "#         shutil.move(train_dir / category / fname,\n",
    "#                     val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d36f0f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Letâ€™s create three Dataset objects for training, validation, and testing:\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2036ac00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'It never ceases to amaze me how you can take an excellent actor, and put him to waste in a film such as this. Robert De Niro is one of the best Hollywood stars of all time, but even he couldn\\'t save this movie. In fact, his character is much the same as the one he played in Cape Fear, which was actually pretty good, but I can\\'t stand it when actors do the same schtick over more than one movie. I believe it gets old, and that is the case here.<br /><br />There\\'s nothing surprising in this movie, but then, the story has been told a million times before. Wesley Snipes is your typical baseball player, and his conceit shows through in his characterization. De Niro plays the obsessed fan, but his role in this film is less than entertaining.<br /><br />However, because De Niro is IN this film, that makes it a draw if you are a fan (no pun intended) who sees everything he does no matter how bad. But to see De Niro at his best, see \"Midnight Run\", \"Goodfellas\", or \"Cop Land\", or even go way back and check out \"Taxi Driver\" or \"Godfather II\". Don\\'t waste your time with this drivel.<br /><br />My Rating: 3/10', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Displaying the shapes and dtypes of the first batch\n",
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f40ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model-building utility\n",
    "  \n",
    "def get_model(max_tokens=maxTokens, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d22312ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the TextVectorization layer to return bigrams with TF-IDF\n",
    "text_vectorizationTfIdf = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=maxTokens,\n",
    "    output_mode=\"tf-idf\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18fa297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.5199 - accuracy: 0.7596 - val_loss: 0.3482 - val_accuracy: 0.8454\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3323 - accuracy: 0.8702 - val_loss: 0.3880 - val_accuracy: 0.8548\n",
      "782/782 [==============================] - 56s 70ms/step - loss: 0.3373 - accuracy: 0.8477\n",
      "Test acc: 0.848\n"
     ]
    }
   ],
   "source": [
    "# Training and testing the TF-IDF bigram model\n",
    "text_vectorizationTfIdf.adapt(text_only_train_ds)\n",
    " \n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorizationTfIdf(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorizationTfIdf(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorizationTfIdf(x), y),\n",
    "    num_parallel_calls=4)\n",
    " \n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=epochCount,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"tfidf_2gram test accuracy: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45410da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/plermant/git/miniprojects/NLPpractice/models/bagBigramTdIdf\\assets\n"
     ]
    }
   ],
   "source": [
    "# Saving/loading model\n",
    "path=nlpRoot+'/models'+'/bagBigramTdIdf'\n",
    "model.save(path)\n",
    "model=keras.models.load_model(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f6398a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelFromName(name):\n",
    "    path=nlpRoot+\"/models/\"+name\n",
    "    if name=='bagBigramTdIdf'or name=='embedBidiLSTM':\n",
    "        return tf.keras.models.load_model(path) \n",
    "    else:\n",
    "        print(\"Error in model name, does not exist, aborting ...\")\n",
    "        exit(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70ab1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSentiment(inString,modelName):\n",
    "    model=modelFromName(modelName)\n",
    "    inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    if modelName=='bagBigramTdIdf':\n",
    "        processed_inputs = text_vectorizationTfIdf(inputs)\n",
    "    elif modelName=='embedBidiLSTM':\n",
    "        processed_inputs = text_vectorizationBidiLSTM(inputs)\n",
    "    else:\n",
    "        print(\"Can't vectorize this model, exiting ..\")\n",
    "        exit(1)\n",
    "    outputs = model(processed_inputs)\n",
    "    inference_model = keras.Model(inputs, outputs)\n",
    "\n",
    "    predictions = inference_model(raw_text_data) \n",
    "    resp=round(float(predictions[0] * 100),2)\n",
    "    return resp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a75096cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               73984     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 284s 445ms/step - loss: 0.3847 - accuracy: 0.8298 - val_loss: 0.2789 - val_accuracy: 0.8844\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 279s 446ms/step - loss: 0.2264 - accuracy: 0.9118 - val_loss: 0.2840 - val_accuracy: 0.8866\n",
      "782/782 [==============================] - 121s 152ms/step - loss: 0.2908 - accuracy: 0.8790\n",
      "bidiLSTM test accuracy: 0.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/plermant/git/miniprojects/NLPpractice/models/embedBidiLSTM\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/plermant/git/miniprojects/NLPpractice/models/embedBidiLSTM\\assets\n"
     ]
    }
   ],
   "source": [
    "# Now lets do embedding with bidi LSTM\n",
    "  \n",
    "text_vectorizationBidiLSTM = layers.TextVectorization(\n",
    "    max_tokens=maxTokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorizationBidiLSTM.adapt(text_only_train_ds)\n",
    " \n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorizationBidiLSTM(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorizationBidiLSTM(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorizationBidiLSTM(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(\n",
    "    input_dim=maxTokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=epochCount,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\") \n",
    "print(f\"bidiLSTM test accuracy: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
    "\n",
    "#Saving/loading model\n",
    "path=nlpRoot+'/models'+'/embedBidiLSTM'\n",
    "model.save(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c91bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.27 percent positive\n",
      "91.58 percent positive\n"
     ]
    }
   ],
   "source": [
    "raw_text_data = tf.convert_to_tensor([[\"That was an excellent movie, I loved it.\"],])\n",
    "#raw_text_data = tf.convert_to_tensor([[\"That was a horrible movie, I hated it.\"],])\n",
    "\n",
    "print(predictSentiment(raw_text_data,'bagBigramTdIdf'),\"percent positive for bagBigramTdIdf\")\n",
    "print(predictSentiment(raw_text_data,'embedBidiLSTM'),\"percent positive for embedBidiLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46f87c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
